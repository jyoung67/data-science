{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Means Clustering\n",
    "\n",
    "Today, we're going to implement our own k-Means clustering function. We will use this function, and another clustering model from scikit-learn, to replicate the results of Ahlquist & Breunig (2012). The data for this paper has been provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We'll need _at least_ the following modules. You may use additional modules if they are helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's implement k-Means\n",
    "\n",
    "Remember, the k-Means algorithm consists of the following steps:\n",
    "\n",
    "1. Randomly initialize k points within the range of your data.\n",
    "    * Hint: you can do this either by randomly sampling points from within the bounds of all the variables you'll be clustering _or_ you can randomly sample values from your actual data.\n",
    "2. Compute the distance from every cluster to every point in your data.\n",
    "3. Find the nearest cluster to every point.\n",
    "4. For all the points in each cluster, compute their mean value.\n",
    "5. Use the newly-found mean values as your new cluster centroids.\n",
    "6. Repeat X times (usually 10-20).\n",
    "\n",
    "## A note on notation\n",
    "\n",
    "* `data` is your dataset - an (n x p) numpy array\n",
    "* `centroids` are the means of each cluster, a (k x p) numpy array\n",
    "* `n` is the number of datapoints in your data\n",
    "* `k` is the number of clusters\n",
    "* `p` is the number of variables per data point\n",
    "* `x` and `y` represent two singular datapoints\n",
    "* `dists` represents an (n x k) numpy array of distances\n",
    "* `iterations` is the number of iterations to run the model\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "Complete the below functions. The red \"docstrings\" at the top of each function tells you precisely what each function should input and output. It also describes what the function should accomplish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x,y):\n",
    "    \"\"\"\n",
    "    This function computes sqrt((x-y)^2) for two points, x and y.\n",
    "    Inputs\n",
    "        x: a (1 x p) numpy array\n",
    "        y: a (1 x p) numpy array\n",
    "    Outputs\n",
    "        A single numerical value, the distance between x and y.\n",
    "    \"\"\"\n",
    "    distance = np.sqrt(np.sum((x - y)**2))\n",
    "\n",
    "    return distance\n",
    "\n",
    "def get_distances(data, centroids):\n",
    "    \"\"\"\n",
    "    This function computes the distances from every point to every\n",
    "    cluster centroid.\n",
    "    Inputs\n",
    "        data: an (n x p) numpy array of data points\n",
    "        centroids: a (k x p) numpy array of cluster centers\n",
    "    Outputs\n",
    "        An (n x k) numpy array of distances from points to clusters.\n",
    "    \"\"\"\n",
    "    distance_matrix = np.empty((data.shape[0], centroids.shape[0]))\n",
    "    \n",
    "    for i in range(0,data.shape[0]):\n",
    "        for j in range(0,centroids.shape[0]):\n",
    "            distance_matrix[i,j] = euclidean_distance(data[i], centroids[j])\n",
    "\n",
    "    return distance_matrix\n",
    "    \n",
    "def get_clusters(dists):\n",
    "    \"\"\"\n",
    "    This function computes the cluster assignment for all points.\n",
    "    Inputs\n",
    "        dists: an (n x k) numpy array\n",
    "    Outputs\n",
    "        An (n x 1) numpy array of datapoints by cluster assignments.\n",
    "    \"\"\"\n",
    "    cluster_assignments = np.argmin(dists, axis = 1)\n",
    "    ## Hint: np.argmin will help...\n",
    "    \n",
    "    return cluster_assignments\n",
    "\n",
    "def compute_new_centroids(data, cluster_assignments):\n",
    "    \"\"\"\n",
    "    This function computes the new centroids (means) for each cluster.\n",
    "    Inputs\n",
    "        data: an (n x p) numpy array of your data points\n",
    "        cluster_assignments: an (n x 1) numpy array giving the nearest \n",
    "            cluster to each point in your dataset.\n",
    "    Outputs\n",
    "        A (k x p) numpy array of cluster centroids (means).\n",
    "    \"\"\"\n",
    "    uniqueClusterIndexes = np.unique(cluster_assignments)\n",
    "    sz = uniqueClusterIndexes.size\n",
    "    centroids = np.repeat([np.NaN,np.NaN], sz, axis = 0).reshape(sz,2)\n",
    "    for i, idx in enumerate(uniqueClusterIndexes):\n",
    "        currentDataIndexes = np.argwhere(cluster_assignments == idx)\n",
    "        sh = currentDataIndexes.shape[0]\n",
    "        if sh > 0:\n",
    "            x = np.mean(data[currentDataIndexes.reshape(sh,)][:,0])\n",
    "            y = np.mean(data[currentDataIndexes.reshape(sh,)][:,1])\n",
    "            centroids[i] = [x,y]  \n",
    "\n",
    "    return centroids\n",
    "\n",
    "\n",
    "\n",
    "def kmeans(data, k=3, iterations=10):\n",
    "    \"\"\"\n",
    "    Runs the k-Means algorithm.\n",
    "    Inputs\n",
    "        data: an (n x p) numpy array representing your data\n",
    "        k: the number of clusters to estimate (1 < k <= n)\n",
    "        iterations: the number of cycles to run the algorithm\n",
    "    Outputs\n",
    "        A tuple of (cluster_assignments, centroids) where:\n",
    "         * cluster_assignments is a numpy array of (n x 1)\n",
    "         * centroids is a numpy array of (k x p)\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Initialize k randomly-selected points for centroids.\n",
    "    ## You might find np.random.choice to help here.\n",
    "    \n",
    "    \n",
    "    \n",
    "    #centroids = data[np.random.choice(data.shape[0], k, replace=False)].copy()\n",
    "    centroids = np.random.normal(loc=5, scale=2, size=(k,data.shape[1]))    \n",
    "    print(\"Initial Centroids:\", centroids)\n",
    "    \n",
    "    ## Loop for _iterations_ times.\n",
    "    for ii in range(iterations):        \n",
    "        \n",
    "        ## 1. Compute distances between data and centroids\n",
    "        distances = get_distances(data, centroids)\n",
    "        \n",
    "        ## 2. Get cluster assignment for each datapoint\n",
    "        cluster_assignments = get_clusters(distances)       \n",
    "        \n",
    "        ## 3. Compute new cluster centroids\n",
    "        tmpClusters = compute_new_centroids(data, cluster_assignments)\n",
    "        for i, newCentroidIndex in enumerate(np.unique(cluster_assignments)):\n",
    "            centroids[newCentroidIndex] = tmpClusters[i]        \n",
    "        \n",
    "    return (cluster_assignments, centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.34, 2.9]\n",
      "[5.0, 10.833333333333334]\n",
      "[10.7, 8.05]\n",
      "Initial Centroids: [[5.92226318 8.54944788]\n",
      " [3.6768688  4.51165808]\n",
      " [5.71814875 5.6937139 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0]), array([[7.28      , 9.72      ],\n",
       "        [3.34      , 2.9       ],\n",
       "        [5.71814875, 5.6937139 ]]))"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input = np.array([[3.7, 2.4], [3.0, 2.5], [3.4, 3.3], [3.8,3.0],[2.8,3.3],[5.0, 10.3],[5.5,10.9],[4.5,11.3],[10.2,7.5], [11.2,8.6]])\n",
    "print([np.mean(data[:5, 0]), np.mean(data[:5, 1])])\n",
    "print([np.mean(data[5:8, 0]), np.mean(data[5:8, 1])])\n",
    "print([np.mean(data[8:10, 0]), np.mean(data[8:10, 1])])\n",
    "kmeans(data_input, 3, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.7  2.4]\n",
      " [ 3.   2.5]\n",
      " [ 3.4  3.3]\n",
      " [ 3.8  3. ]\n",
      " [ 2.8  3.3]\n",
      " [ 5.  10.3]\n",
      " [ 5.5 10.9]\n",
      " [ 4.5 11.3]\n",
      " [10.2  7.5]\n",
      " [11.2  8.6]]\n",
      "[3.34, 2.9]\n",
      "[5.0, 10.833333333333334]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([10.2, 11.2])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([[3.7, 2.4], [3.0, 2.5], [3.4, 3.3], [3.8,3.0],[2.8,3.3],[5.0, 10.3],[5.5,10.9],[4.5,11.3],[10.2,7.5], [11.2,8.6]])\n",
    "print(data)\n",
    "print([np.mean(data[:5, 0]), np.mean(data[:5, 1])])\n",
    "print([np.mean(data[5:8, 0]), np.mean(data[5:8, 1])])\n",
    "print([np.mean(data[8:10, 0]), np.mean(data[8:10, 1])])\n",
    "data[8:10, 0]\n",
    "#centroids = np.array([[2.0, 4.0],[4.0, 12.0], [10.0, 9.0]])\n",
    "#distances = get_distances(data, centroids)\n",
    "#print(distances)\n",
    "#cluster_assignments = get_clusters(distances)\n",
    "#compute_new_centroids(data, cluster_assignments)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.65765602, -8.93976053],\n",
       "       [-8.20947713, -0.69803999],\n",
       "       [13.0010013 , 12.02775926]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data[np.random.choice(10, 3, replace=False)].copy().shape\n",
    "np.random.choice(50, 2, replace=False)\n",
    "np.random.normal(loc=5, scale=10, size=(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.7  2.4]\n",
      " [ 3.   2.5]\n",
      " [ 3.4  3.3]\n",
      " [ 3.8  3. ]\n",
      " [ 2.8  3.3]\n",
      " [ 5.  10.3]\n",
      " [ 5.5 10.9]\n",
      " [ 4.5 11.3]\n",
      " [10.2  7.5]\n",
      " [11.2  8.6]]\n",
      "[0 0 0 0 0 1 1 1 2 2]\n",
      "[0 1 2]\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[3.7, 2.4], [3.0, 2.5], [3.4, 3.3], [3.8,3.0],[2.8,3.3],[5.0, 10.3],[5.5,10.9],[4.5,11.3],[10.2,7.5], [11.2,8.6]])\n",
    "print(data)\n",
    "centroids = np.array([[2.0, 4.0],[4.0, 12.0], [10.0, 9.0]])\n",
    "distances = get_distances(data, centroids)\n",
    "clusterAssignments = get_clusters(distances)\n",
    "print(clusterAssignments)\n",
    "uniqueClusterIndexes = np.unique(clusterAssignments)\n",
    "sz = uniqueClusterIndexes.size\n",
    "newCentroids = np.repeat([np.NaN,np.NaN], sz, axis = 0).reshape(sz,2)\n",
    "\n",
    "\n",
    "print(uniqueClusterIndexes)\n",
    "for i, idx in enumerate(uniqueClusterIndexes):    \n",
    "    currentDataIndexes = np.argwhere(clusterAssignments==idx)\n",
    "    sh = currentDataIndexes.shape[0]\n",
    "    if sh > 0:\n",
    "        x = np.mean(data[currentDataIndexes.reshape(sh,)][:,0])\n",
    "        y = np.mean(data[currentDataIndexes.reshape(sh,)][:,1])\n",
    "        newCentroids[i] = [x,y]\n",
    "        \n",
    "print(newCentroids.shape)\n",
    "        \n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[8, 13,  5,  0],[ 0,  2,  5,  3],[10,  7, 15, 15],[ 3, 11,  4, 12]])\n",
    "print(arr)\n",
    "#print(\"\\nIndices of min element : \", np.argmin(arr, axis = 1).shape) \n",
    "rrr = np.empty((arr.shape[0], ), dtype=np.int64)\n",
    "result = np.argmin(arr, axis = 1)\n",
    "print(result)\n",
    "\n",
    "print(result.reshape(4,1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Let's test our kmeans function with the Iris data we've used before. I've given you a plotting function to make it easy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categories(X, Y, category):\n",
    "    fig, ax = plt.subplots()\n",
    "    unique_categories = np.unique(category)\n",
    "    for cat in unique_categories:\n",
    "        ax.plot(X[category==cat], Y[category==cat], marker='o', linestyle='', label=cat)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will read the Iris dataset and use just the `PetalLength` and `PetalWidth` features (columns). Below, I have provided code to plot these two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv(\"iris.csv\")\n",
    "features = np.array(iris[[\"PetalLength\",\"PetalWidth\"]])\n",
    "plot_categories(features[:,0], features[:,1], iris[\"Species\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, see how well your k-means algorithm does. First, let's use `k=3` and `iterations=1`. Compute the results and make a new plot colored by cluster assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Now, increase `iterations` to 10 and plot the results. How do they compare to the true values from the Iris dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Just for fun, let's see what happens if we increase `k` to 6. Plot the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "Let's replicate Ahlquist & Breunig (2012) Figure 3a. I've gotten you started (below) by loading the data and adding the \"ground truth\" VOC values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "data = pd.read_csv(\"iversen.csv\")\n",
    "countries = [\"AUS\", \"CAN\",\"GBR\",\n",
    "            \"JPN\",\"CHE\",\"USA\",\n",
    "            \"AUT\",\"BEL\",\"DEU\",\n",
    "            \"FRA\",\"ITA\",\"DNK\",\n",
    "            \"FIN\",\"NLD\",\"NOR\",\n",
    "            \"SWE\",\"GRC\", \"IRL\", \n",
    "            \"NZL\",\"PRT\",\"ESP\"]\n",
    "VoC_class = [\"LME\",\"LME\",\"LME\",\n",
    "             \"CME\",\"CME\",\"LME\",\n",
    "             \"CME\",\"CME\",\"CME\",\n",
    "             \"NC\",\"NC\",\"CME\",\n",
    "             \"CME\",\"CME\",\"CME\",\n",
    "             \"CME\",\"NC\",\"LME\",\n",
    "             \"LME\",\"NC\", \"NC\"]\n",
    "VoC_color = {\"CME\":\"red\",\"LME\":\"black\",\"NC\":\"green\"}\n",
    "\n",
    "voc_data = pd.DataFrame(list(zip(countries,VoC_class)), columns=[\"wbcode\",\"voc\"])\n",
    "\n",
    "merged_data = pd.merge(data, voc_data, on=\"wbcode\")\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use sklearn's `StandardScaler` to scale our variables to have 0 mean and 1 variance. You can find the documentation for this at https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_employment = StandardScaler()\n",
    "scaled_employment_data = scaler_employment.fit_transform(merged_data[[\"epl\",\"cdp\",\"cop\"]])\n",
    "\n",
    "scaler_unemployment = StandardScaler()\n",
    "scaled_unemployment_data = scaler_unemployment.fit_transform(merged_data[[\"nurr\",\"gb\",\"dsj\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use sklearn's `PCA` to compute principal components that represent employment and unemployment variables. This process is described in section 4.1.1 of the paper. We stack these into an `X` matrix of size (n x 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_pca = PCA(n_components=1)\n",
    "employment_pca_values = employment_pca.fit_transform(scaled_employment_data)\n",
    "\n",
    "unemployment_pca = PCA(n_components=1)\n",
    "unemployment_pca_values = unemployment_pca.fit_transform(scaled_unemployment_data)\n",
    "\n",
    "X = -np.hstack([employment_pca_values, unemployment_pca_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use both your k-means implementation and sklearn's `GaussianMixture` to reproduce Figure 3(a) of the paper. You do not need to draw the ellipses around the clusters, just color the points by cluster like we did above. For the Gaussian Mixture model, use `covariance_type='tied'`. Also, you should use `k=5` for your k-means and `n_components=5` for the Gaussian Mixture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "Now that you've estimated a Gaussian Mixture model, you will need to make predictions for each datapoint. That is, predict the cluster that every datapoint belongs to. You can use the `.predict(...)` method of your Gaussian Mixture object to do so. See the documentation if it is unclear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "\n",
    "Plot your results for k-means and for the gaussian mixture model. You can use the same function we used for the iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8\n",
    "\n",
    "Select a dataset of your choice (any...) and use clustering (either k-means or any of the clustering options available from [scikit-learn](https://scikit-learn.org/stable/modules/clustering.html)) to explore that dataset. Prepare to discuss your data, your clustering choices, and the results for approximately 5-10 minutes on March 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
