{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fearon & Laitin Replication\n",
    "\n",
    "Today, we're going to see if we can do a better job than Fearon & Laitin do in predicting the onset of civil wars. You may use any method you like, but I will get you started using their method (logistic regression) and also random forests.\n",
    "\n",
    "To evaluate how well we do, we will use out-of-sample testing. I will help you divide the data into 3 sets: Training, Validation, and Testing. You should fit a model on the training data _only_ and evaluate how well it performs out-of-sample on the _validation_ data. Only once you have selected a _final_ model should you evaluate it's performance on the _test_ set. Once you've \"peeked\" at the test set, you should no longer continue to alter your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"FearonLaitin.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Scikit learn likes to fit models to data in the form of two numpy arrays or pandas matrices. The first one, `X`, is a matrix of all your predictor variables (also known as independent variables). The second one, `y`, is the column vector of your dependent variables. If `X` is $(n \\times k)$ dimensions, `y` is $(n \\times 1)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, I subset the data to just thos variables we need.\n",
    "# Then I drop all rows with NA values.\n",
    "all_variables = [\"onset\",\"warl\",\"gdpenl\",\"lpop\",\"lmtnest\",\"ncontig\",\"Oil\",\"nwstate\",\"instab\",\"polity2l\",\"ethfrac\",\"relfrac\"]\n",
    "data = data[all_variables].dropna()\n",
    "data = data[data.onset != 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make two new dataframes. One called `X_matrix` with only the `x_variables` and one called `Y_matrix` with only the `y_variables`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variables = [\"warl\",\"gdpenl\",\"lpop\",\"lmtnest\",\"ncontig\",\"Oil\",\"nwstate\",\"instab\",\"polity2l\",\"ethfrac\",\"relfrac\"]\n",
    "y_variables = \"onset\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Use scikit learn's `model_selection.train_test_split` function to create a training set (called `train_X` and `train_Y`) and a hold-out set (called `holdout_X` and `holdout_Y`). You can find the documentation for this function here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "Both the test set and the training set should be one half of the overall data. Set the random state for your split as `1234`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Now, let's split the holdout data in half to produce `test_X`, `test_Y`, `valid_X`, and `valid_Y`. Use the same random state as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Let's see if we can (relatively closely) replicate the model from Fearon & Laitin. They used logistic regression. Let's start by using _all_ of our data (training, test, and validation) combined to replicate their exact Model 1 from Table 1 (p. 84). \n",
    "\n",
    "Use sckit learn's `LogisticRegression` function with the following parameters:\n",
    "\n",
    "* `penalty = \"none\"`\n",
    "* `random_state=0`\n",
    "* `max_iter=1000`\n",
    "* `class_weight=\"balanced\"`\n",
    "\n",
    "Name your model `logit_all`. You can then use the `.fit(X_matrix, Y_matrix)` method to estimate the model.\n",
    "\n",
    "Once you have an estimated model, use `print(logit_all.coef_)` to print the estimated coefficients. They should closely (but not exactly) match the results in model 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "Let's see how well the model predicts civil war onset in-sample. Remember, we haven't held any data out for model evaluation. We trained on the same data that we're evaluating with. \n",
    "\n",
    "Start by making a new variable called `logit_all_pred` that represents your class predictions for each example in `X_matrix`. You can use the `logit_all.predict(...)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use scikit learn's `classification_report()` function to evaluate how well the model predicts civil war onset. Use the `print()` function to make sure the report prints out nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, follow along with the example given in block 16 of https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html to create a confusion matrix to visually represent predictive performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "Let's repeat the above process the right way. Make a new logit model called `logit_model` using just the `train_X` and `train_Y`. Keep all other hyperparameters the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create predictions for your new model, called `logit_pred`, using the `valid_X` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the classification report for `logit_pred` compared to the true values, `valid_Y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "\n",
    "As you can see, the Fearon & Laitin model doesn't do a particularly great job predicting civil war onsets. If we've done everything right, the classification accuracy should be around 67%. That's not horrible, but the false positive rate is very high.\n",
    "\n",
    "Let's now try to model this using a random forest classifier. We will use the `RandomForestClassifier` function from scikit learn. Make a new object called `rf_model` with the following hyperparemeters:\n",
    "\n",
    "* `n_estimators=100`\n",
    "* `max_depth=3`\n",
    "* `random_state=1234`\n",
    "\n",
    "Then, fit your model to `train_X` and `train_Y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8\n",
    "\n",
    "We can check out which features contribute most to our predictive power. Use the `.feature_importances_` attribute to print the weights assigned to each feature. These should sum to 1, meaning they are proportions. If you want, you `zip(...)` them with `x_variables` so it is easy to see which variable is associated with which feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9\n",
    "\n",
    "Use the `predict` method again to create `rf_pred`, the predictions for the validation set from your random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10\n",
    "\n",
    "Print both the classification report and the confusion matrix for your random forest model using the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11\n",
    "\n",
    "Adjust the model as you feel necessary and evaluate its performance on the validation data. You can choose any evaluation metric you like. Once you are satisfied with your model's performance on the validation data, evaluate its performance on the test data. Report the performance of your model on the test data. If you'd like, you can use scikit learn's `GridSearchCV` or related functions to help you pick a \"best\" model. This is a hard problem. If you can't beat the above model, don't stress. It is really really really hard to predict civil war onsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 12\n",
    "\n",
    "Pick a dataset of your choice (from your research or from Googling) and apply a random forest to it. Be prepared to describe the problem that you picked, your modeling choices, and the results of your model. Did it perform better or worse than you expected? Why? What could be done to improve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
