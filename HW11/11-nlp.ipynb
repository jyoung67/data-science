{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Today, we are going to explore using text as data. We will cover the following topics (each one very briefly):\n",
    "\n",
    "1. Preprocessing text (i.e. cleaning text)\n",
    "2. Topic modeling with LDA\n",
    "3. Word vectors using word2vec\n",
    "\n",
    "To get started, please install `gensim`. You can do so using:\n",
    "\n",
    "```\n",
    "conda install -c anaconda gensim\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Let's import the usual suspects: `pandas`, `matplotlib`, `numpy`, and our new favorite library, `gensim`.\n",
    "\n",
    "Gensim is a very powerful module for performing all sorts of natural language processing. It has become the default for word embedding (word vector) models like word2vec and doc2vec. Because `gensim` is very large, we won't import the whole thing. We'll only import the parts that we're going to need.\n",
    "\n",
    "For many problems, you may want to refer to the Gensim documentation. This page will be particularly helpful: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## For preprocessing\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "\n",
    "## For topic modeling\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "## For word embedding models\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Use Pandas to read our `twitter.csv` dataset. Name the new dataframe `docs`. Use `head()` to inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5113 entries, 0 to 5112\n",
      "Data columns (total 5 columns):\n",
      "Topic        5113 non-null object\n",
      "Sentiment    5113 non-null object\n",
      "TweetId      5113 non-null int64\n",
      "TweetDate    5113 non-null object\n",
      "TweetText    5113 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 199.9+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>TweetDate</th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126415614616154112</td>\n",
       "      <td>Tue Oct 18 21:53:25 +0000 2011</td>\n",
       "      <td>Now all @Apple has to do is get swype on the i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126404574230740992</td>\n",
       "      <td>Tue Oct 18 21:09:33 +0000 2011</td>\n",
       "      <td>@Apple will be adding more carrier support to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126402758403305474</td>\n",
       "      <td>Tue Oct 18 21:02:20 +0000 2011</td>\n",
       "      <td>Hilarious @youtube video - guy does a duet wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126397179614068736</td>\n",
       "      <td>Tue Oct 18 20:40:10 +0000 2011</td>\n",
       "      <td>@RIM you made it too easy for me to switch to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126395626979196928</td>\n",
       "      <td>Tue Oct 18 20:34:00 +0000 2011</td>\n",
       "      <td>I just realized that the reason I got into twi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic Sentiment             TweetId                       TweetDate  \\\n",
       "0  apple  positive  126415614616154112  Tue Oct 18 21:53:25 +0000 2011   \n",
       "1  apple  positive  126404574230740992  Tue Oct 18 21:09:33 +0000 2011   \n",
       "2  apple  positive  126402758403305474  Tue Oct 18 21:02:20 +0000 2011   \n",
       "3  apple  positive  126397179614068736  Tue Oct 18 20:40:10 +0000 2011   \n",
       "4  apple  positive  126395626979196928  Tue Oct 18 20:34:00 +0000 2011   \n",
       "\n",
       "                                           TweetText  \n",
       "0  Now all @Apple has to do is get swype on the i...  \n",
       "1  @Apple will be adding more carrier support to ...  \n",
       "2  Hilarious @youtube video - guy does a duet wit...  \n",
       "3  @RIM you made it too easy for me to switch to ...  \n",
       "4  I just realized that the reason I got into twi...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = pd.read_csv(\"twitter.csv\")\n",
    "print(docs.info())\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Wow. That's some messy text. What happens if we use the gensim preprocessing functions we imported above to clean it up a little bit? Go ahead and try it. Make a list called `clean_tweets` and perform the following operations on `docs[\"TweetText\"]`, storing the results each time in `clean_tweets`.\n",
    "\n",
    "Print the first three elements of `clean_tweets` each time to see how things are changing.\n",
    "\n",
    "### Hint\n",
    "\n",
    "The Gensim functions expect they will only get one string (i.e. tweet) at a time. Therefore, you must use list comprehensions to process your list of `clean_tweets`. For example: \n",
    "```python\n",
    "clean_tweets = [function(tweet) for tweet in clean_tweets]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2a\n",
    "Strip all punctuation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Now all  Apple has to do is get swype on the iphone and it will be crack  Iphone that is\n",
      "2)  Apple will be adding more carrier support to the iPhone 4S  just announced \n",
      "3) Hilarious  youtube video   guy does a duet with  apple  s Siri  Pretty much sums up the love affair  http t co 8ExbnQjY\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "clean_tweets = [strip_punctuation(tweet) for tweet in docs.TweetText]\n",
    "for i in range(3):\n",
    "    print(str(i+1) + \")\", clean_tweets[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2b\n",
    "\n",
    "Strip multiple whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Now all Apple has to do is get swype on the iphone and it will be crack Iphone that is\n",
      "2)  Apple will be adding more carrier support to the iPhone 4S just announced \n",
      "3) Hilarious youtube video guy does a duet with apple s Siri Pretty much sums up the love affair http t co 8ExbnQjY\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "clean_tweets = [strip_multiple_whitespaces(tweet) for tweet in clean_tweets]\n",
    "for i in range(3):\n",
    "    print(str(i+1) + \")\", clean_tweets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2c\n",
    "\n",
    "Strip numeric values from the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Now all Apple has to do is get swype on the iphone and it will be crack Iphone that is\n",
      "2)  Apple will be adding more carrier support to the iPhone S just announced \n",
      "3) Hilarious youtube video guy does a duet with apple s Siri Pretty much sums up the love affair http t co ExbnQjY\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "clean_tweets = [strip_numeric(tweet) for tweet in clean_tweets]\n",
    "for i in range(3):\n",
    "    print(str(i+1) + \")\", clean_tweets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2d\n",
    "\n",
    "Strip all of the English stopwords from the tweets. A stopword is a common word that does not add value to our data. For example: \"The\", \"a\", \"were\", \"are\", \"be\", \"is\", \"there\" are all stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Now Apple swype iphone crack Iphone\n",
      "2) Apple adding carrier support iPhone S announced\n",
      "3) Hilarious youtube video guy duet apple s Siri Pretty sums love affair http t ExbnQjY\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "clean_tweets = [remove_stopwords(tweet) for tweet in clean_tweets]\n",
    "for i in range(3):\n",
    "    print(str(i+1) + \")\", clean_tweets[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2e\n",
    "\n",
    "Finally, let's drop all of the words less than three characters in length. Use `strip_short` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Now Apple swype iphone crack Iphone\n",
      "2) Apple adding carrier support iPhone announced\n",
      "3) Hilarious youtube video guy duet apple Siri Pretty sums love affair http ExbnQjY\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import strip_short\n",
    "clean_tweets = [strip_short(tweet, minsize=3) for tweet in clean_tweets]\n",
    "for i in range(3):\n",
    "    print(str(i+1) + \")\", clean_tweets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Many functions in Gensim expect the corpus (collection of texts) to be a lists of lists. Our corpus is our list of tweets. However, right now that is simply a list full of strings. Each string is a tweet (or \"document\"). We need to transform our corpus into a list of lists. The outer list contains all of the documents. The inner lists each represent a document. They contain strings. Each string is an individual word. For example:\n",
    "\n",
    "```\n",
    "[\n",
    "  [\"docA_word1\",\"docA_word2\",\"docA_word3\"],\n",
    "  [\"docB_word1\",\"docB_word2\",\"docB_word3\",\"docB_word4\"],\n",
    "  ...,\n",
    "  [\"docZ_word1\",\"docZ_word2\"]\n",
    "]\n",
    "```\n",
    "\n",
    "Again, use a list comprehension to split every document. You can use the built-in `split()` method to split a single string into a list of words. The default split behavior is to divide the sentence up based on whitespace. Call your new corpus `tokenized_docs`. \n",
    "\n",
    "Tokenization is the process of splitting a string (or document or text) into a collection of tokens (typically words).\n",
    "\n",
    "Print the first three elements of your `tokenized_docs` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) ['Now', 'Apple', 'swype', 'iphone', 'crack', 'Iphone']\n",
      "2) ['Apple', 'adding', 'carrier', 'support', 'iPhone', 'announced']\n",
      "3) ['Hilarious', 'youtube', 'video', 'guy', 'duet', 'apple', 'Siri', 'Pretty', 'sums', 'love', 'affair', 'http', 'ExbnQjY']\n"
     ]
    }
   ],
   "source": [
    "clean_tweets = [str.split(tweet) for tweet in clean_tweets]\n",
    "for i in range(3):\n",
    "    print(str(i+1) + \")\", clean_tweets[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Once we have our corpus represented as a list of lists and our documents are all tokenized, we can use Gensim's `Dictionary` function to make a dictionary that represents the vocabulary of our corpus. Do that below. Call your dictionary `dictionary`. \n",
    "\n",
    "If you then `print(dictionary)`, it will show you some of your terms and also tell you how many unique tokens are in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(16840 unique tokens: ['Apple', 'Iphone', 'Now', 'crack', 'iphone']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(clean_tweets)\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "Now we need to convert each document into a new data structure called a bag of words. A bag of words representation of a document is a vector the same length as the number of words in your vocabulary (or dictionary). If you have 10,000 unique words, every document will be represented by a length 10,000 vector. This vector is 0 for all words that _do not_ appear in a document. For every word that _does_ appear in that document, the corresponding entry in the bag of words vector is the _count_ of the number of times that word appears in the document.\n",
    "\n",
    "Use the `doc2bow` method of your `dictionary` to convert every document in your corpus into a bag of words.\n",
    "\n",
    "(Again, see here: https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "\n",
    "Call your new list `tweets_bow`. Because Gensim uses an efficient method to store the bag of words data, printing this object will not be very useful or insightful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(7607, 1), (9605, 1)],\n",
       " [(41, 1), (298, 1), (994, 1), (3288, 1)],\n",
       " [(41, 1), (7607, 1)],\n",
       " [(9605, 1)],\n",
       " [(41, 1), (298, 1), (3288, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(994, 1)]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "tweets_bow = [dictionary.doc2bow(text) for text in common_texts]\n",
    "tweets_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "Now, use `gensim`'s `LdaModel` to estimate a topic model of our tweets with `num_topics=4`. Call your model `lda`. \n",
    "\n",
    "Use the following hyperparameters or arguments:\n",
    "\n",
    "* `num_topics=4`\n",
    "* `id2word=dictionary`\n",
    "* `passes=5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "\n",
    "Use the `print_topics` method to print the top terms for each of your topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8\n",
    "\n",
    "Let's make predictions for which topic each of our Tweets is in. Use the `get_document_topics()` method to get the topic distribution for all of our tweets.\n",
    "\n",
    "1. First, give `get_document_topics()` your `tweets_bow` object. Call the output of this function `topics`.\n",
    "2. This returns a complicated list of list of tuples. It looks like below:\n",
    "\n",
    "```\n",
    "  [\n",
    "    [ (0, Pr(t=0)), (1, Pr(t=1)),..., (3, Pr(t=3)) ],\n",
    "    ...\n",
    "    [ (0, Pr(t=0)), (1, Pr(t=1)),..., (3, Pr(t=3)) ]\n",
    "  ]\n",
    "```\n",
    "3. Write a loop that iterates over `topics` and finds the highest probability topic for each document. Record the topic number with the highest probability for every tweet in a new list called `best_topics`. You can also do this with a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9\n",
    "\n",
    "We know the \"true\" topic for every tweet. It is in `docs[\"Topic\"]`. These were hand-labeled. We did not tell our model _anything_ about these labels. Let's see if it was able to discern these four topics.\n",
    "\n",
    "Make a contingency matrix of `docs[\"Topic\"]` and `best_topics`. Print the contingency matrix. You do not need to plot it visually. The easiest way to do this is going to be the following:\n",
    "\n",
    "1. Make a new dictionary that contains keys \"actual\" and \"pred\".\n",
    "2. The value of \"actual\" should be `docs[\"Topic\"].tolist()`.\n",
    "3. The value of \"pred\" should be `best_topics`.\n",
    "4. Use the pandas `DataFrame` method to turn your dictionary into a pandas dataframe.\n",
    "5. Now, use the `pd.crosstab` method with `margins=False` to print your contingency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10\n",
    "\n",
    "Now, let's learn a word vector model using word2vec. Word vectors are real-valued representations of words that retain the words' meanings. Words with similar meanings will have similar word vectors. For example, `dog` will be closer to `wolf` than it is to `house`. We can also perform algebra on word vectors. The common example is:\n",
    "\n",
    "$$\n",
    "vector(king) + vector(woman) - vector(man) \\approx vector(queen)\n",
    "$$\n",
    "\n",
    "The first step is to read in a somewhat larger and more interesting dataset on hotel reviews. Use pandas' to load `hotel_reviews.csv` into a dataframe called `hotels`. Print the head of `hotels`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11\n",
    "\n",
    "Next, preprocess the text of each review using `gensim` following the same steps as in Problem 2 and Problem 3. You should finish with a list of tokenized documents (each document is a hotel review). You can call this `tokenized_reviews`.  \n",
    "\n",
    "This time, also convert all of the texts to lowercase. You can accomplish this by using the `.lower()` method as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"This is AN eXaMPLe\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 12\n",
    "\n",
    "Use gensim's `Word2Vec` to train a word2vec model called `w2v`. Set the following arguments:\n",
    "\n",
    "* `size=100`\n",
    "* `min_count=5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 13\n",
    "\n",
    "Use the `most_similar` method to find the words that are most similar to \"clean\". Also use the function to find the words that are most similar to \"dirty\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 14\n",
    "\n",
    "Use the `most_similar` method to find words that satisfy the following equation:\n",
    "\n",
    "$$\n",
    "best + bad - good = ?\n",
    "$$\n",
    "\n",
    "This correspond to the anaology:\n",
    "\n",
    "```\n",
    "good:best::bad:?\n",
    "```\n",
    "\n",
    "You can use the `positive` and `negative` arguments of `most_similar` to construct your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
