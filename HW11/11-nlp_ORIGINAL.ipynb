{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Today, we are going to explore using text as data. We will cover the following topics (each one very briefly):\n",
    "\n",
    "1. Preprocessing text (i.e. cleaning text)\n",
    "2. Topic modeling with LDA\n",
    "3. Word vectors using word2vec\n",
    "\n",
    "To get started, please install `gensim`. You can do so using:\n",
    "\n",
    "```\n",
    "conda install -c anaconda gensim\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Let's import the usual suspects: `pandas`, `matplotlib`, `numpy`, and our new favorite library, `gensim`.\n",
    "\n",
    "Gensim is a very powerful module for performing all sorts of natural language processing. It has become the default for word embedding (word vector) models like word2vec and doc2vec. Because `gensim` is very large, we won't import the whole thing. We'll only import the parts that we're going to need.\n",
    "\n",
    "For many problems, you may want to refer to the Gensim documentation. This page will be particularly helpful: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## For preprocessing\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "\n",
    "## For topic modeling\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "## For word embedding models\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Use Pandas to read our `twitter.csv` dataset. Name the new dataframe `docs`. Use `head()` to inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Wow. That's some messy text. What happens if we use the gensim preprocessing functions we imported above to clean it up a little bit? Go ahead and try it. Make a list called `clean_tweets` and perform the following operations on `docs[\"TweetText\"]`, storing the results each time in `clean_tweets`.\n",
    "\n",
    "Print the first three elements of `clean_tweets` each time to see how things are changing.\n",
    "\n",
    "### Hint\n",
    "\n",
    "The Gensim functions expect they will only get one string (i.e. tweet) at a time. Therefore, you must use list comprehensions to process your list of `clean_tweets`. For example: \n",
    "```python\n",
    "clean_tweets = [function(tweet) for tweet in clean_tweets]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2a\n",
    "Strip all punctuation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2b\n",
    "\n",
    "Strip multiple whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2c\n",
    "\n",
    "Strip numeric values from the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2d\n",
    "\n",
    "Strip all of the English stopwords from the tweets. A stopword is a common word that does not add value to our data. For example: \"The\", \"a\", \"were\", \"are\", \"be\", \"is\", \"there\" are all stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2e\n",
    "\n",
    "Finally, let's drop all of the words less than three characters in length. Use `strip_short` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Many functions in Gensim expect the corpus (collection of texts) to be a lists of lists. Our corpus is our list of tweets. However, right now that is simply a list full of strings. Each string is a tweet (or \"document\"). We need to transform our corpus into a list of lists. The outer list contains all of the documents. The inner lists each represent a document. They contain strings. Each string is an individual word. For example:\n",
    "\n",
    "```\n",
    "[\n",
    "  [\"docA_word1\",\"docA_word2\",\"docA_word3\"],\n",
    "  [\"docB_word1\",\"docB_word2\",\"docB_word3\",\"docB_word4\"],\n",
    "  ...,\n",
    "  [\"docZ_word1\",\"docZ_word2\"]\n",
    "]\n",
    "```\n",
    "\n",
    "Again, use a list comprehension to split every document. You can use the built-in `split()` method to split a single string into a list of words. The default split behavior is to divide the sentence up based on whitespace. Call your new corpus `tokenized_docs`. \n",
    "\n",
    "Tokenization is the process of splitting a string (or document or text) into a collection of tokens (typically words).\n",
    "\n",
    "Print the first three elements of your `tokenized_docs` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Once we have our corpus represented as a list of lists and our documents are all tokenized, we can use Gensim's `Dictionary` function to make a dictionary that represents the vocabulary of our corpus. Do that below. Call your dictionary `dictionary`. \n",
    "\n",
    "If you then `print(dictionary)`, it will show you some of your terms and also tell you how many unique tokens are in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "Now we need to convert each document into a new data structure called a bag of words. A bag of words representation of a document is a vector the same length as the number of words in your vocabulary (or dictionary). If you have 10,000 unique words, every document will be represented by a length 10,000 vector. This vector is 0 for all words that _do not_ appear in a document. For every word that _does_ appear in that document, the corresponding entry in the bag of words vector is the _count_ of the number of times that word appears in the document.\n",
    "\n",
    "Use the `doc2bow` method of your `dictionary` to convert every document in your corpus into a bag of words.\n",
    "\n",
    "(Again, see here: https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "\n",
    "Call your new list `tweets_bow`. Because Gensim uses an efficient method to store the bag of words data, printing this object will not be very useful or insightful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "Now, use `gensim`'s `LdaModel` to estimate a topic model of our tweets with `num_topics=4`. Call your model `lda`. \n",
    "\n",
    "Use the following hyperparameters or arguments:\n",
    "\n",
    "* `num_topics=4`\n",
    "* `id2word=dictionary`\n",
    "* `passes=5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "\n",
    "Use the `print_topics` method to print the top terms for each of your topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8\n",
    "\n",
    "Let's make predictions for which topic each of our Tweets is in. Use the `get_document_topics()` method to get the topic distribution for all of our tweets.\n",
    "\n",
    "1. First, give `get_document_topics()` your `tweets_bow` object. Call the output of this function `topics`.\n",
    "2. This returns a complicated list of list of tuples. It looks like below:\n",
    "\n",
    "```\n",
    "  [\n",
    "    [ (0, Pr(t=0)), (1, Pr(t=1)),..., (3, Pr(t=3)) ],\n",
    "    ...\n",
    "    [ (0, Pr(t=0)), (1, Pr(t=1)),..., (3, Pr(t=3)) ]\n",
    "  ]\n",
    "```\n",
    "3. Write a loop that iterates over `topics` and finds the highest probability topic for each document. Record the topic number with the highest probability for every tweet in a new list called `best_topics`. You can also do this with a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9\n",
    "\n",
    "We know the \"true\" topic for every tweet. It is in `docs[\"Topic\"]`. These were hand-labeled. We did not tell our model _anything_ about these labels. Let's see if it was able to discern these four topics.\n",
    "\n",
    "Make a contingency matrix of `docs[\"Topic\"]` and `best_topics`. Print the contingency matrix. You do not need to plot it visually. The easiest way to do this is going to be the following:\n",
    "\n",
    "1. Make a new dictionary that contains keys \"actual\" and \"pred\".\n",
    "2. The value of \"actual\" should be `docs[\"Topic\"].tolist()`.\n",
    "3. The value of \"pred\" should be `best_topics`.\n",
    "4. Use the pandas `DataFrame` method to turn your dictionary into a pandas dataframe.\n",
    "5. Now, use the `pd.crosstab` method with `margins=False` to print your contingency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10\n",
    "\n",
    "Now, let's learn a word vector model using word2vec. Word vectors are real-valued representations of words that retain the words' meanings. Words with similar meanings will have similar word vectors. For example, `dog` will be closer to `wolf` than it is to `house`. We can also perform algebra on word vectors. The common example is:\n",
    "\n",
    "$$\n",
    "vector(king) + vector(woman) - vector(man) \\approx vector(queen)\n",
    "$$\n",
    "\n",
    "The first step is to read in a somewhat larger and more interesting dataset on hotel reviews. Use pandas' to load `hotel_reviews.csv` into a dataframe called `hotels`. Print the head of `hotels`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11\n",
    "\n",
    "Next, preprocess the text of each review using `gensim` following the same steps as in Problem 2 and Problem 3. You should finish with a list of tokenized documents (each document is a hotel review). You can call this `tokenized_reviews`.  \n",
    "\n",
    "This time, also convert all of the texts to lowercase. You can accomplish this by using the `.lower()` method as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"This is AN eXaMPLe\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 12\n",
    "\n",
    "Use gensim's `Word2Vec` to train a word2vec model called `w2v`. Set the following arguments:\n",
    "\n",
    "* `size=100`\n",
    "* `min_count=5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 13\n",
    "\n",
    "Use the `most_similar` method to find the words that are most similar to \"clean\". Also use the function to find the words that are most similar to \"dirty\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 14\n",
    "\n",
    "Use the `most_similar` method to find words that satisfy the following equation:\n",
    "\n",
    "$$\n",
    "best + bad - good = ?\n",
    "$$\n",
    "\n",
    "This correspond to the anaology:\n",
    "\n",
    "```\n",
    "good:best::bad:?\n",
    "```\n",
    "\n",
    "You can use the `positive` and `negative` arguments of `most_similar` to construct your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:grad6009]",
   "language": "python",
   "name": "conda-env-grad6009-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
