{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Today, we are going to explore using text as data. We will cover the following topics (each one very briefly):\n",
    "\n",
    "1. Preprocessing text (i.e. cleaning text)\n",
    "2. Topic modeling with LDA\n",
    "3. Word vectors using word2vec\n",
    "\n",
    "To get started, please install `gensim`. You can do so using:\n",
    "\n",
    "```\n",
    "conda install -c anaconda gensim\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Let's import the usual suspects: `pandas`, `matplotlib`, `numpy`, and our new favorite library, `gensim`.\n",
    "\n",
    "Gensim is a very powerful module for performing all sorts of natural language processing. It has become the default for word embedding (word vector) models like word2vec and doc2vec. Because `gensim` is very large, we won't import the whole thing. We'll only import the parts that we're going to need.\n",
    "\n",
    "For many problems, you may want to refer to the Gensim documentation. This page will be particularly helpful: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## For preprocessing\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "\n",
    "## For topic modeling\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "## For word embedding models\n",
    "from gensim.models import Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "Use Pandas to read our `twitter.csv` dataset. Name the new dataframe `docs`. Use `head()` to inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>TweetId</th>\n",
       "      <th>TweetDate</th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126415614616154112</td>\n",
       "      <td>Tue Oct 18 21:53:25 +0000 2011</td>\n",
       "      <td>Now all @Apple has to do is get swype on the i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126404574230740992</td>\n",
       "      <td>Tue Oct 18 21:09:33 +0000 2011</td>\n",
       "      <td>@Apple will be adding more carrier support to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126402758403305474</td>\n",
       "      <td>Tue Oct 18 21:02:20 +0000 2011</td>\n",
       "      <td>Hilarious @youtube video - guy does a duet wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126397179614068736</td>\n",
       "      <td>Tue Oct 18 20:40:10 +0000 2011</td>\n",
       "      <td>@RIM you made it too easy for me to switch to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple</td>\n",
       "      <td>positive</td>\n",
       "      <td>126395626979196928</td>\n",
       "      <td>Tue Oct 18 20:34:00 +0000 2011</td>\n",
       "      <td>I just realized that the reason I got into twi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic Sentiment             TweetId                       TweetDate  \\\n",
       "0  apple  positive  126415614616154112  Tue Oct 18 21:53:25 +0000 2011   \n",
       "1  apple  positive  126404574230740992  Tue Oct 18 21:09:33 +0000 2011   \n",
       "2  apple  positive  126402758403305474  Tue Oct 18 21:02:20 +0000 2011   \n",
       "3  apple  positive  126397179614068736  Tue Oct 18 20:40:10 +0000 2011   \n",
       "4  apple  positive  126395626979196928  Tue Oct 18 20:34:00 +0000 2011   \n",
       "\n",
       "                                           TweetText  \n",
       "0  Now all @Apple has to do is get swype on the i...  \n",
       "1  @Apple will be adding more carrier support to ...  \n",
       "2  Hilarious @youtube video - guy does a duet wit...  \n",
       "3  @RIM you made it too easy for me to switch to ...  \n",
       "4  I just realized that the reason I got into twi...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = pd.read_csv(\"twitter.csv\")\n",
    "docs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Wow. That's some messy text. What happens if we use the gensim preprocessing functions we imported above to clean it up a little bit? Go ahead and try it. Make a list called `clean_tweets` and perform the following operations on `docs[\"TweetText\"]`, storing the results each time in `clean_tweets`.\n",
    "\n",
    "Print the first three elements of `clean_tweets` each time to see how things are changing.\n",
    "\n",
    "### Hint\n",
    "\n",
    "The Gensim functions expect they will only get one string (i.e. tweet) at a time. Therefore, you must use list comprehensions to process your list of `clean_tweets`. For example: \n",
    "```python\n",
    "clean_tweets = [function(tweet) for tweet in clean_tweets]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2a\n",
    "Strip all punctuation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Now all  Apple has to do is get swype on the iphone and it will be crack  Iphone that is', ' Apple will be adding more carrier support to the iPhone 4S  just announced ', 'Hilarious  youtube video   guy does a duet with  apple  s Siri  Pretty much sums up the love affair  http t co 8ExbnQjY']\n"
     ]
    }
   ],
   "source": [
    "clean_tweets = docs[\"TweetText\"]\n",
    "clean_tweets = [strip_punctuation(a) for a in clean_tweets]\n",
    "print(clean_tweets[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2b\n",
    "\n",
    "Strip multiple whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Now all Apple has to do is get swype on the iphone and it will be crack Iphone that is', ' Apple will be adding more carrier support to the iPhone 4S just announced ', 'Hilarious youtube video guy does a duet with apple s Siri Pretty much sums up the love affair http t co 8ExbnQjY']\n"
     ]
    }
   ],
   "source": [
    "clean_tweets = [strip_multiple_whitespaces(a) for a in clean_tweets]\n",
    "print(clean_tweets[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2c\n",
    "\n",
    "Strip numeric values from the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Now all Apple has to do is get swype on the iphone and it will be crack Iphone that is', ' Apple will be adding more carrier support to the iPhone S just announced ', 'Hilarious youtube video guy does a duet with apple s Siri Pretty much sums up the love affair http t co ExbnQjY']\n"
     ]
    }
   ],
   "source": [
    "clean_tweets = [strip_numeric(a) for a in clean_tweets]\n",
    "print(clean_tweets[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2d\n",
    "\n",
    "Strip all of the English stopwords from the tweets. A stopword is a common word that does not add value to our data. For example: \"The\", \"a\", \"were\", \"are\", \"be\", \"is\", \"there\" are all stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Now Apple swype iphone crack Iphone', 'Apple adding carrier support iPhone S announced', 'Hilarious youtube video guy duet apple s Siri Pretty sums love affair http t ExbnQjY']\n"
     ]
    }
   ],
   "source": [
    "clean_tweets = [remove_stopwords(a) for a in clean_tweets]\n",
    "print(clean_tweets[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2e\n",
    "\n",
    "Finally, let's drop all of the words less than three characters in length. Use `strip_short` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Now Apple swype iphone crack Iphone', 'Apple adding carrier support iPhone announced', 'Hilarious youtube video guy duet apple Siri Pretty sums love affair http ExbnQjY']\n"
     ]
    }
   ],
   "source": [
    "clean_tweets = [strip_short(a) for a in clean_tweets]\n",
    "print(clean_tweets[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Many functions in Gensim expect the corpus (collection of texts) to be a lists of lists. Our corpus is our list of tweets. However, right now that is simply a list full of strings. Each string is a tweet (or \"document\"). We need to transform our corpus into a list of lists. The outer list contains all of the documents. The inner lists each represent a document. They contain strings. Each string is an individual word. For example:\n",
    "\n",
    "```\n",
    "[\n",
    "  [\"docA_word1\",\"docA_word2\",\"docA_word3\"],\n",
    "  [\"docB_word1\",\"docB_word2\",\"docB_word3\",\"docB_word4\"],\n",
    "  ...,\n",
    "  [\"docZ_word1\",\"docZ_word2\"]\n",
    "]\n",
    "```\n",
    "\n",
    "Again, use a list comprehension to split every document. You can use the built-in `split()` method to split a single string into a list of words. The default split behavior is to divide the sentence up based on whitespace. Call your new corpus `tokenized_docs`. \n",
    "\n",
    "Tokenization is the process of splitting a string (or document or text) into a collection of tokens (typically words).\n",
    "\n",
    "Print the first three elements of your `tokenized_docs` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Now', 'Apple', 'swype', 'iphone', 'crack', 'Iphone'], ['Apple', 'adding', 'carrier', 'support', 'iPhone', 'announced'], ['Hilarious', 'youtube', 'video', 'guy', 'duet', 'apple', 'Siri', 'Pretty', 'sums', 'love', 'affair', 'http', 'ExbnQjY']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_docs = [a.split() for a in clean_tweets]\n",
    "print(tokenized_docs[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Once we have our corpus represented as a list of lists and our documents are all tokenized, we can use Gensim's `Dictionary` function to make a dictionary that represents the vocabulary of our corpus. Do that below. Call your dictionary `dictionary`. \n",
    "\n",
    "If you then `print(dictionary)`, it will show you some of your terms and also tell you how many unique tokens are in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1464 unique tokens: ['Apple', 'Iphone', 'Now', 'iphone', 'announced']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(tokenized_docs)\n",
    "dictionary.filter_extremes()\n",
    "print(dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "\n",
    "Now we need to convert each document into a new data structure called a bag of words. A bag of words representation of a document is a vector the same length as the number of words in your vocabulary (or dictionary). If you have 10,000 unique words, every document will be represented by a length 10,000 vector. This vector is 0 for all words that _do not_ appear in a document. For every word that _does_ appear in that document, the corresponding entry in the bag of words vector is the _count_ of the number of times that word appears in the document.\n",
    "\n",
    "Use the `doc2bow` method of your `dictionary` to convert every document in your corpus into a bag of words.\n",
    "\n",
    "(Again, see here: https://radimrehurek.com/gensim/models/ldamodel.html)\n",
    "\n",
    "Call your new list `tweets_bow`. Because Gensim uses an efficient method to store the bag of words data, printing this object will not be very useful or insightful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_bow = [dictionary.doc2bow(a) for a in tokenized_docs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "\n",
    "Now, use `gensim`'s `LdaModel` to estimate a topic model of our tweets with `num_topics=4`. Call your model `lda`. \n",
    "\n",
    "Use the following hyperparameters or arguments:\n",
    "\n",
    "* `num_topics=4`\n",
    "* `id2word=dictionary`\n",
    "* `passes=5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(tweets_bow, num_topics=4, id2word=dictionary, passes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "\n",
    "Use the `print_topics` method to print the top terms for each of your topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.124*\"Microsoft\" + 0.110*\"http\" + 0.042*\"microsoft\" + 0.018*\"Twitter\" + 0.018*\"Windows\" + 0.013*\"Ballmer\" + 0.012*\"que\" + 0.011*\"Apple\" + 0.010*\"Steve\" + 0.010*\"Phone\" + 0.008*\"mas\" + 0.008*\"como\" + 0.008*\"esta\" + 0.008*\"para\" + 0.007*\"Yahoo\" + 0.007*\"Free\" + 0.006*\"OFF\" + 0.006*\"free\" + 0.006*\"Android\" + 0.006*\"Que\"'),\n",
       " (1,\n",
       "  '0.109*\"http\" + 0.083*\"Google\" + 0.039*\"Android\" + 0.026*\"google\" + 0.022*\"Nexus\" + 0.019*\"Samsung\" + 0.018*\"Sandwich\" + 0.018*\"Cream\" + 0.018*\"Ice\" + 0.018*\"Galaxy\" + 0.017*\"Apple\" + 0.012*\"android\" + 0.011*\"The\" + 0.010*\"apple\" + 0.009*\"los\" + 0.008*\"iPhone\" + 0.007*\"todo\" + 0.007*\"Facebook\" + 0.007*\"las\" + 0.005*\"ICS\"'),\n",
       " (2,\n",
       "  '0.053*\"apple\" + 0.039*\"google\" + 0.023*\"http\" + 0.018*\"Apple\" + 0.016*\"new\" + 0.012*\"followers\" + 0.011*\"android\" + 0.010*\"nexusprime\" + 0.009*\"iOS\" + 0.009*\"iPhone\" + 0.008*\"Siri\" + 0.008*\"meu\" + 0.008*\"app\" + 0.008*\"Follow\" + 0.007*\"time\" + 0.007*\"know\" + 0.007*\"like\" + 0.007*\"phone\" + 0.007*\"tweets\" + 0.006*\"ICS\"'),\n",
       " (3,\n",
       "  '0.177*\"twitter\" + 0.137*\"Twitter\" + 0.035*\"que\" + 0.019*\"TWITTER\" + 0.017*\"por\" + 0.016*\"http\" + 0.012*\"Facebook\" + 0.011*\"facebook\" + 0.008*\"para\" + 0.008*\"people\" + 0.007*\"autopilot\" + 0.007*\"lol\" + 0.006*\"like\" + 0.006*\"let\" + 0.005*\"Off\" + 0.005*\"pero\" + 0.005*\"got\" + 0.004*\"aday\" + 0.004*\"cuando\" + 0.004*\"aqui\"')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8\n",
    "\n",
    "Let's make predictions for which topic each of our Tweets is in. Use the `get_document_topics()` method to get the topic distribution for all of our tweets.\n",
    "\n",
    "1. First, give `get_document_topics()` your `tweets_bow` object. Call the output of this function `topics`.\n",
    "2. This returns a complicated list of list of tuples. It looks like below:\n",
    "\n",
    "```\n",
    "  [\n",
    "    [ (0, Pr(t=0)), (1, Pr(t=1)),..., (3, Pr(t=3)) ],\n",
    "    ...\n",
    "    [ (0, Pr(t=0)), (1, Pr(t=1)),..., (3, Pr(t=3)) ]\n",
    "  ]\n",
    "```\n",
    "3. Write a loop that iterates over `topics` and finds the highest probability topic for each document. Record the topic number with the highest probability for every tweet in a new list called `best_topics`. You can also do this with a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = lda.get_document_topics(tweets_bow)\n",
    "\n",
    "best_topics = []\n",
    "for doc in topics:\n",
    "    current_probability = 0\n",
    "    current_topic = 0\n",
    "    for tup in doc:\n",
    "        if tup[1] > current_probability:\n",
    "            current_topic = tup[0]\n",
    "            current_probability = tup[1]\n",
    "    best_topics.append(current_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9\n",
    "\n",
    "We know the \"true\" topic for every tweet. It is in `docs[\"Topic\"]`. These were hand-labeled. We did not tell our model _anything_ about these labels. Let's see if it was able to discern these four topics.\n",
    "\n",
    "Make a contingency matrix of `docs[\"Topic\"]` and `best_topics`. Print the contingency matrix. You do not need to plot it visually. The easiest way to do this is going to be the following:\n",
    "\n",
    "1. Make a new dictionary that contains keys \"actual\" and \"pred\".\n",
    "2. The value of \"actual\" should be `docs[\"Topic\"].tolist()`.\n",
    "3. The value of \"pred\" should be `best_topics`.\n",
    "4. Use the pandas `DataFrame` method to turn your dictionary into a pandas dataframe.\n",
    "5. Now, use the `pd.crosstab` method with `margins=False` to print your contingency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>apple</th>\n",
       "      <td>95</td>\n",
       "      <td>215</td>\n",
       "      <td>780</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>google</th>\n",
       "      <td>26</td>\n",
       "      <td>838</td>\n",
       "      <td>411</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>microsoft</th>\n",
       "      <td>1067</td>\n",
       "      <td>131</td>\n",
       "      <td>96</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter</th>\n",
       "      <td>119</td>\n",
       "      <td>61</td>\n",
       "      <td>161</td>\n",
       "      <td>949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "pred          0    1    2    3\n",
       "actual                        \n",
       "apple        95  215  780   52\n",
       "google       26  838  411   42\n",
       "microsoft  1067  131   96   70\n",
       "twitter     119   61  161  949"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "df = pd.DataFrame({\"actual\":docs[\"Topic\"].tolist(), \"pred\":best_topics})\n",
    "\n",
    "pd.crosstab(df[\"actual\"], df[\"pred\"], margins=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10\n",
    "\n",
    "Now, let's learn a word vector model using word2vec. Word vectors are real-valued representations of words that retain the words' meanings. Words with similar meanings will have similar word vectors. For example, `dog` will be closer to `wolf` than it is to `house`. We can also perform algebra on word vectors. The common example is:\n",
    "\n",
    "$$\n",
    "vector(king) + vector(woman) - vector(man) \\approx vector(queen)\n",
    "$$\n",
    "\n",
    "The first step is to read in a somewhat larger and more interesting dataset on hotel reviews. Use pandas' to load `hotel_reviews.csv` into a dataframe called `hotels`. Print the head of `hotels`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>province</th>\n",
       "      <th>reviews.date</th>\n",
       "      <th>reviews.dateAdded</th>\n",
       "      <th>reviews.doRecommend</th>\n",
       "      <th>reviews.id</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "      <th>reviews.userCity</th>\n",
       "      <th>reviews.username</th>\n",
       "      <th>reviews.userProvince</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Riviera San Nicol 11/a</td>\n",
       "      <td>Hotels</td>\n",
       "      <td>Mableton</td>\n",
       "      <td>US</td>\n",
       "      <td>45.421611</td>\n",
       "      <td>12.376187</td>\n",
       "      <td>Hotel Russo Palace</td>\n",
       "      <td>30126</td>\n",
       "      <td>GA</td>\n",
       "      <td>2013-09-22T00:00:00Z</td>\n",
       "      <td>2016-10-24T00:00:25Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Pleasant 10 min walk along the sea front to th...</td>\n",
       "      <td>Good location away from the crouds</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russ (kent)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Riviera San Nicol 11/a</td>\n",
       "      <td>Hotels</td>\n",
       "      <td>Mableton</td>\n",
       "      <td>US</td>\n",
       "      <td>45.421611</td>\n",
       "      <td>12.376187</td>\n",
       "      <td>Hotel Russo Palace</td>\n",
       "      <td>30126</td>\n",
       "      <td>GA</td>\n",
       "      <td>2015-04-03T00:00:00Z</td>\n",
       "      <td>2016-10-24T00:00:25Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Really lovely hotel. Stayed on the very top fl...</td>\n",
       "      <td>Great hotel with Jacuzzi bath!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A Traveler</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Riviera San Nicol 11/a</td>\n",
       "      <td>Hotels</td>\n",
       "      <td>Mableton</td>\n",
       "      <td>US</td>\n",
       "      <td>45.421611</td>\n",
       "      <td>12.376187</td>\n",
       "      <td>Hotel Russo Palace</td>\n",
       "      <td>30126</td>\n",
       "      <td>GA</td>\n",
       "      <td>2014-05-13T00:00:00Z</td>\n",
       "      <td>2016-10-24T00:00:25Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Ett mycket bra hotell. Det som drog ner betyge...</td>\n",
       "      <td>Lugnt lï¿½ï¿½ge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Maud</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Riviera San Nicol 11/a</td>\n",
       "      <td>Hotels</td>\n",
       "      <td>Mableton</td>\n",
       "      <td>US</td>\n",
       "      <td>45.421611</td>\n",
       "      <td>12.376187</td>\n",
       "      <td>Hotel Russo Palace</td>\n",
       "      <td>30126</td>\n",
       "      <td>GA</td>\n",
       "      <td>2013-10-27T00:00:00Z</td>\n",
       "      <td>2016-10-24T00:00:25Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>We stayed here for four nights in October. The...</td>\n",
       "      <td>Good location on the Lido.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Julie</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Riviera San Nicol 11/a</td>\n",
       "      <td>Hotels</td>\n",
       "      <td>Mableton</td>\n",
       "      <td>US</td>\n",
       "      <td>45.421611</td>\n",
       "      <td>12.376187</td>\n",
       "      <td>Hotel Russo Palace</td>\n",
       "      <td>30126</td>\n",
       "      <td>GA</td>\n",
       "      <td>2015-03-05T00:00:00Z</td>\n",
       "      <td>2016-10-24T00:00:25Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>We stayed here for four nights in October. The...</td>\n",
       "      <td>ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sungchul</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  address categories      city country   latitude  longitude  \\\n",
       "0  Riviera San Nicol 11/a     Hotels  Mableton      US  45.421611  12.376187   \n",
       "1  Riviera San Nicol 11/a     Hotels  Mableton      US  45.421611  12.376187   \n",
       "2  Riviera San Nicol 11/a     Hotels  Mableton      US  45.421611  12.376187   \n",
       "3  Riviera San Nicol 11/a     Hotels  Mableton      US  45.421611  12.376187   \n",
       "4  Riviera San Nicol 11/a     Hotels  Mableton      US  45.421611  12.376187   \n",
       "\n",
       "                 name postalCode province          reviews.date  \\\n",
       "0  Hotel Russo Palace      30126       GA  2013-09-22T00:00:00Z   \n",
       "1  Hotel Russo Palace      30126       GA  2015-04-03T00:00:00Z   \n",
       "2  Hotel Russo Palace      30126       GA  2014-05-13T00:00:00Z   \n",
       "3  Hotel Russo Palace      30126       GA  2013-10-27T00:00:00Z   \n",
       "4  Hotel Russo Palace      30126       GA  2015-03-05T00:00:00Z   \n",
       "\n",
       "      reviews.dateAdded  reviews.doRecommend  reviews.id  reviews.rating  \\\n",
       "0  2016-10-24T00:00:25Z                  NaN         NaN             4.0   \n",
       "1  2016-10-24T00:00:25Z                  NaN         NaN             5.0   \n",
       "2  2016-10-24T00:00:25Z                  NaN         NaN             5.0   \n",
       "3  2016-10-24T00:00:25Z                  NaN         NaN             5.0   \n",
       "4  2016-10-24T00:00:25Z                  NaN         NaN             5.0   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  Pleasant 10 min walk along the sea front to th...   \n",
       "1  Really lovely hotel. Stayed on the very top fl...   \n",
       "2  Ett mycket bra hotell. Det som drog ner betyge...   \n",
       "3  We stayed here for four nights in October. The...   \n",
       "4  We stayed here for four nights in October. The...   \n",
       "\n",
       "                                       reviews.title reviews.userCity  \\\n",
       "0                 Good location away from the crouds              NaN   \n",
       "1                     Great hotel with Jacuzzi bath!              NaN   \n",
       "2                                    Lugnt lï¿½ï¿½ge              NaN   \n",
       "3                         Good location on the Lido.              NaN   \n",
       "4  ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½...              NaN   \n",
       "\n",
       "  reviews.username reviews.userProvince  \n",
       "0      Russ (kent)                  NaN  \n",
       "1       A Traveler                  NaN  \n",
       "2             Maud                  NaN  \n",
       "3            Julie                  NaN  \n",
       "4         sungchul                  NaN  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels = pd.read_csv(\"hotel_reviews.csv\")\n",
    "hotels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11\n",
    "\n",
    "Next, preprocess the text of each review using `gensim` following the same steps as in Problem 2 and Problem 3. You should finish with a list of tokenized documents (each document is a hotel review). You can call this `tokenized_reviews`.  \n",
    "\n",
    "This time, also convert all of the texts to lowercase. You can accomplish this by using the `.lower()` method as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is an example'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"This is AN eXaMPLe\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['pleasant', 'min', 'walk', 'sea', 'water', 'bus', 'restaurants', 'hotel', 'comfortable', 'breakfast', 'good', 'variety', 'room', 'aircon', 'work', 'mosquito', 'repelant'], ['lovely', 'hotel', 'stayed', 'floor', 'surprised', 'jacuzzi', 'bath', 'know', 'getting', 'staff', 'friendly', 'helpful', 'included', 'breakfast', 'great', 'great', 'location', 'great', 'value', 'money', 'want', 'leave'], ['ett', 'mycket', 'bra', 'hotell', 'det', 'som', 'drog', 'ner', 'betyget', 'var', 'att', 'fick', 'ett', 'rum', 'taksarna', 'det', 'endast', 'var', 'sthjd', 'rummets', 'yta']]\n"
     ]
    }
   ],
   "source": [
    "clean_reviews = hotels[\"reviews.text\"]\n",
    "clean_reviews = [strip_punctuation(str(a).lower()) for a in clean_reviews]\n",
    "clean_reviews = [strip_multiple_whitespaces(a) for a in clean_reviews]\n",
    "clean_reviews = [strip_numeric(a) for a in clean_reviews]\n",
    "clean_reviews = [remove_stopwords(a) for a in clean_reviews]\n",
    "clean_reviews = [strip_short(a) for a in clean_reviews]\n",
    "tokenized_reviews = [a.split() for a in clean_reviews]\n",
    "print(tokenized_reviews[0:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 12\n",
    "\n",
    "Use gensim's `Word2Vec` to train a word2vec model called `w2v`. Set the following arguments:\n",
    "\n",
    "* `size=100`\n",
    "* `min_count=5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(tokenized_reviews, size=100, min_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 13\n",
    "\n",
    "Use the `most_similar` method to find the words that are most similar to \"clean\". Also use the function to find the words that are most similar to \"dirty\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spotless', 0.8316045999526978), ('spacious', 0.775711178779602), ('cozy', 0.7689898610115051), ('roomy', 0.766965925693512), ('nice', 0.759313702583313), ('appointed', 0.7583928108215332), ('nicely', 0.7554788589477539), ('updated', 0.7529506683349609), ('neat', 0.7449461221694946), ('complaints', 0.735970675945282)]\n",
      "[('filthy', 0.9525532126426697), ('stained', 0.9291964769363403), ('mold', 0.9284066557884216), ('disgusting', 0.9278249740600586), ('stains', 0.9255205392837524), ('gross', 0.9219987988471985), ('vacuumed', 0.9131742715835571), ('nasty', 0.9078891277313232), ('damp', 0.9041059017181396), ('mildew', 0.9037607908248901)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/grad6009/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "//anaconda3/envs/grad6009/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(w2v.most_similar(\"clean\"))\n",
    "print(w2v.most_similar(\"dirty\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 14\n",
    "\n",
    "Use the `most_similar` method to find words that satisfy the following equation:\n",
    "\n",
    "$$\n",
    "best + bad - good = ?\n",
    "$$\n",
    "\n",
    "This correspond to the anaology:\n",
    "\n",
    "```\n",
    "good:best::bad:?\n",
    "```\n",
    "\n",
    "You can use the `positive` and `negative` arguments of `most_similar` to construct your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/grad6009/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('worst', 0.8165651559829712),\n",
       " ('read', 0.7050925493240356),\n",
       " ('slowest', 0.6925835609436035),\n",
       " ('horrible', 0.6905118227005005),\n",
       " ('won', 0.6849080324172974),\n",
       " ('write', 0.682223916053772),\n",
       " ('star', 0.6713719367980957),\n",
       " ('worse', 0.6658665537834167),\n",
       " ('point', 0.6651700735092163),\n",
       " ('believe', 0.6622961163520813)]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar([\"best\",\"bad\"], negative=[\"good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:grad6009]",
   "language": "python",
   "name": "conda-env-grad6009-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
